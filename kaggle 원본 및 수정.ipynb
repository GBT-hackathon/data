{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9489760,"sourceType":"datasetVersion","datasetId":5773582},{"sourceId":9490030,"sourceType":"datasetVersion","datasetId":5773762},{"sourceId":9586995,"sourceType":"datasetVersion","datasetId":5846609},{"sourceId":9610079,"sourceType":"datasetVersion","datasetId":5863841},{"sourceId":9614670,"sourceType":"datasetVersion","datasetId":5867262},{"sourceId":9620101,"sourceType":"datasetVersion","datasetId":5871365},{"sourceId":9620189,"sourceType":"datasetVersion","datasetId":5871433},{"sourceId":136066,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":115130,"modelId":138394}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/kobert_tokenizer/pytorch/default/1/kobert_tokenizer.py')  # 토크나이저 파일이 있는 경로 추가\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.264445Z","iopub.execute_input":"2024-10-14T13:04:30.264834Z","iopub.status.idle":"2024-10-14T13:04:30.269762Z","shell.execute_reply.started":"2024-10-14T13:04:30.264798Z","shell.execute_reply":"2024-10-14T13:04:30.268875Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom torch.optim.lr_scheduler import StepLR\nfrom sklearn.metrics import f1_score\nfrom tqdm import tqdm\nimport pandas as pd\nfrom types import SimpleNamespace\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport random\nimport numpy as np\n#import KoBERTTokenizer\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.271526Z","iopub.execute_input":"2024-10-14T13:04:30.271866Z","iopub.status.idle":"2024-10-14T13:04:30.283096Z","shell.execute_reply.started":"2024-10-14T13:04:30.271833Z","shell.execute_reply":"2024-10-14T13:04:30.282231Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter","metadata":{}},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"#augmented, 500개로 맞춤\n#augmented2, 1000개로 맞춤\n#augmented_combined, 동일 카테고리별로 5개씩 묶고, 합친 갯수가 200개보다 적으면 200개까지 증강\n#augmented_combined3, 동일 카테고리별로 2개씩 묶고, 합친 갯수가 200개보다 적으면 50개까지 증강\n#augmented_combined4, 셔플 추가, 동일 카테고리별로 2개씩 묶고, 합친 갯수가 200개보다 적으면 50개까지 증강\n#augmented_combined5, 셔플 추가, 동일 카테고리별로 2개씩 묶고, 합친 갯수가 200개보다 적으면 50개까지 증강\n#그리고 지역은 5000개까지 줄임\n#augmented_combined6, 셔플 추가, 동일 카테고리별로 2개씩 묶고, 합친 갯수가 30개보다 적으면 30개까지 증강\n#augmented_combined7, 셔플 추가, 동일 카테고리별로 2개씩 묶고, 합친 갯수가 30개보다 적으면 30개까지 증강\n#그리고 지역은 7000개까지 줄임\n#augmented_combined8, 셔플 추가, 동일 카테고리별로 2개씩 묶고, 합친 갯수가 30개보다 적으면 30개까지 증강\n#그리고 지역은 5000개까지 줄임\n#augmented_combined7, 셔플 추가, 동일 카테고리별로 2개씩 묶고, 합친 갯수가 30개보다 적으면 30개까지 증강\n#그리고 지역은 3000개까지 줄임\n#augmented_combined7, 셔플 추가, 동일 카테고리별로 2개씩 묶고, 합친 갯수가 30개보다 적으면 30개까지 증강\n#그리고 지역은 2000개까지 줄임\n\ntrain_df = pd.read_csv(\"/kaggle/working/editted.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/dataset/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.284033Z","iopub.execute_input":"2024-10-14T13:04:30.284318Z","iopub.status.idle":"2024-10-14T13:04:30.950352Z","shell.execute_reply.started":"2024-10-14T13:04:30.284288Z","shell.execute_reply":"2024-10-14T13:04:30.947476Z"},"trusted":true},"execution_count":11,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#augmented, 500개로 맞춤\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#augmented2, 1000개로 맞춤\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#augmented_combined, 동일 카테고리별로 5개씩 묶고, 합친 갯수가 200개보다 적으면 200개까지 증강\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#augmented_combined7, 셔플 추가, 동일 카테고리별로 2개씩 묶고, 합친 갯수가 30개보다 적으면 30개까지 증강\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#그리고 지역은 2000개까지 줄임\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/editted.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/dataset/test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/editted.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/editted.csv'","output_type":"error"}]},{"cell_type":"code","source":"train_df[\"분류\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.951348Z","iopub.status.idle":"2024-10-14T13:04:30.951875Z","shell.execute_reply.started":"2024-10-14T13:04:30.951605Z","shell.execute_reply":"2024-10-14T13:04:30.951632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#비슷한거 날리고 2개 합침（새로운 형태의 데이터가 필요하면 변경）\n","metadata":{}},{"cell_type":"code","source":"def combine_articles(df):\n    combined_data = []\n    removed_count = 0  # 삭제된 데이터 수를 추적\n    threshold = 0.7  # 유사도 임계값\n\n    for category in df['분류'].unique():\n        category_data = df[df['분류'] == category].reset_index(drop=True)\n\n        # 카테고리 데이터가 10,000개 이상일 경우에만 진행, 지역만 적용됨\n        if len(category_data) > 10000:\n\n            # 제목과 키워드를 합친 텍스트 데이터 생성\n            category_data['combined_text'] = category_data['제목'] + \" \" + category_data['키워드']\n\n            # TF-IDF 벡터화\n            vectorizer = TfidfVectorizer()\n            tfidf_matrix = vectorizer.fit_transform(category_data['combined_text'])\n\n            # 코사인 유사도 계산\n            cosine_sim = cosine_similarity(tfidf_matrix)\n\n            # 유사한 기사 2개씩 묶기 + 일부 중복 제거\n            used_indices = set()\n\n            # 중복 제거를 위해 일정 임계값 이상의 유사도를 가진 데이터 탐색\n            duplicate_indices = []\n            for i in range(len(cosine_sim)):\n                for j in range(i + 1, len(cosine_sim)):\n                    if cosine_sim[i][j] > threshold:\n                        duplicate_indices.append(j)\n\n            # 중복된 데이터의 절반만 제거\n            unique_duplicate_indices = list(set(duplicate_indices))\n            to_remove = random.sample(unique_duplicate_indices, len(unique_duplicate_indices) // 2)\n            removed_count += len(to_remove)  # 삭제된 데이터 수 기록\n\n            # 중복 제거 후, 인덱스가 올바른지 확인\n            for i in range(len(category_data)):\n                if i in used_indices or i in to_remove:\n                    continue\n                \n                # 가장 유사한 기사 선택\n                most_similar_idx = np.argsort(cosine_sim[i])[-2]  # 자기 자신을 제외한 가장 유사한 기사 선택\n                \n                # most_similar_idx가 사용되었거나 제거된 항목인지 확인\n                if most_similar_idx in used_indices or most_similar_idx in to_remove:\n                    continue\n                \n                combined_title = category_data['제목'][i] + \" \" + category_data['제목'][most_similar_idx]\n                combined_keywords = category_data['키워드'][i] + \",\" + category_data['키워드'][most_similar_idx]\n                \n                combined_entry = {\n                    '분류': category,\n                    '제목': combined_title,\n                    '키워드': combined_keywords\n                }\n                \n                combined_data.append(combined_entry)\n                used_indices.add(i)\n                used_indices.add(most_similar_idx)\n\n        else:\n            # 데이터가 10,000개 이하인 경우 2개씩 단순 묶기\n            for i in range(0, len(category_data), 1):\n                combined_title = \" \".join(category_data['제목'][i:i+2])\n                combined_keywords = \",\".join(category_data['키워드'][i:i+2])\n                combined_entry = {\n                    '분류': category,\n                    '제목': combined_title,\n                    '키워드': combined_keywords\n                }\n                combined_data.append(combined_entry)\n\n    # 총 몇 개의 데이터가 합병되었는지 확인 출력\n    print(f\"총 {removed_count}개의 데이터가 유사해서 합병 또는 제거되었습니다.\")\n    \n    combined_df = pd.DataFrame(combined_data)\n\n    # CSV로 저장\n    combined_df.to_csv(\"/kaggle/working/editted.csv\", index=False, encoding='utf-8-sig')\n    print(\"CSV 파일로 저장되었습니다: editted.csv\")\n    \n    return combined_df\n\n# 사용 예시\n# df = pd.DataFrame({'분류': [...], '제목': [...], '키워드': [...]})\n# new_df = combine_articles(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.953574Z","iopub.status.idle":"2024-10-14T13:04:30.954056Z","shell.execute_reply.started":"2024-10-14T13:04:30.953798Z","shell.execute_reply":"2024-10-14T13:04:30.953822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import random\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.metrics.pairwise import cosine_similarity\n\n# def combine_articles(df):\n#     combined_data = []\n#     removed_count = 0  # 삭제된 데이터 수를 추적\n#     threshold = 0.7  # 유사도 임계값\n#     downsampling_limit = 10000  # 과도한 데이터를 가진 카테고리에 대한 제한\n\n#     for category in df['분류'].unique():\n#         category_data = df[df['분류'] == category].reset_index(drop=True)\n\n#         # 제목과 키워드를 합친 텍스트 데이터 생성\n#         category_data['combined_text'] = category_data['제목'] + \" \" + category_data['키워드']\n\n#         if len(category_data) > downsampling_limit:\n#             # TF-IDF 벡터화 (유사도 계산)\n#             vectorizer = TfidfVectorizer()\n#             tfidf_matrix = vectorizer.fit_transform(category_data['combined_text'])\n\n#             # 코사인 유사도 계산\n#             cosine_sim = cosine_similarity(tfidf_matrix)\n\n#             print(f\"{category} 카테고리에서 유사도 기반으로 중복 제거를 진행합니다.\")\n\n#             # 유사한 기사 중 중복 제거\n#             used_indices = set()\n#             duplicate_indices = []\n#             for i in range(len(cosine_sim)):\n#                 for j in range(i + 1, len(cosine_sim)):\n#                     if cosine_sim[i][j] > threshold:\n#                         duplicate_indices.append(j)\n\n#             # 중복된 데이터의 절반만 제거\n#             unique_duplicate_indices = list(set(duplicate_indices))\n#             to_remove = random.sample(unique_duplicate_indices, len(unique_duplicate_indices) // 2)\n#             removed_count += len(to_remove)  # 삭제된 데이터 수 기록\n\n#             # 중복 제거 후 남은 데이터\n# #             category_data = category_data.drop(to_remove).reset_index(drop=True)\n# #             print(f\"{category} 카테고리에서 {len(to_remove)}개의 유사한 데이터가 제거되었습니다.\")\n\n#             # 유사도 기반으로 중복 제거 후 데이터가 10,000개 이상인 경우 랜덤 다운샘플링\n# #             if len(category_data) > downsampling_limit:\n# #                 category_data = category_data.sample(n=downsampling_limit, random_state=42).reset_index(drop=True)\n# #                 print(f\"{category} 카테고리에서 {len(category_data)}개의 데이터로 랜덤 다운샘플링되었습니다.\")\n        \n#         # 유사도 계산을 하지 않고, 10,000개 이하일 때 2개씩 묶기\n#         else:\n#             used_indices = set()\n#             for i in range(0, len(category_data), 2):\n#                 if i + 1 < len(category_data):\n#                     combined_title = category_data['제목'][i] + \" \" + category_data['제목'][i+1]\n#                     combined_keywords = category_data['키워드'][i] + \",\" + category_data['키워드'][i+1]\n#                 else:\n#                     # 마지막에 남은 데이터는 단독으로 추가\n#                     combined_title = category_data['제목'][i]\n#                     combined_keywords = category_data['키워드'][i]\n\n#                 combined_entry = {\n#                     '분류': category,\n#                     '제목': combined_title,\n#                     '키워드': combined_keywords\n#                 }\n\n#                 combined_data.append(combined_entry)\n\n#     # 총 몇 개의 데이터가 합병되었는지 확인 출력\n#     print(f\"총 {removed_count}개의 데이터가 유사해서 합병 또는 제거되었습니다.\")\n\n#     combined_df = pd.DataFrame(combined_data)\n\n#     # CSV로 저장\n#     combined_df.to_csv(\"/kaggle/working/editted.csv\", index=False, encoding='utf-8-sig')\n#     print(\"CSV 파일로 저장되었습니다: editted.csv\")\n\n#     return combined_df\n\n# # 사용 예시\n# # df = pd.DataFrame({'분류': [...], '제목': [...], '키워드': [...]})\n# # new_df = combine_articles(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.955588Z","iopub.status.idle":"2024-10-14T13:04:30.956069Z","shell.execute_reply.started":"2024-10-14T13:04:30.955819Z","shell.execute_reply":"2024-10-14T13:04:30.955844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combined_df = combine_articles(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.957317Z","iopub.status.idle":"2024-10-14T13:04:30.957671Z","shell.execute_reply.started":"2024-10-14T13:04:30.957495Z","shell.execute_reply":"2024-10-14T13:04:30.957513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final_df=combined_df","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.959160Z","iopub.status.idle":"2024-10-14T13:04:30.959488Z","shell.execute_reply.started":"2024-10-14T13:04:30.959322Z","shell.execute_reply":"2024-10-14T13:04:30.959339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df=train_df","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.960554Z","iopub.status.idle":"2024-10-14T13:04:30.960901Z","shell.execute_reply.started":"2024-10-14T13:04:30.960724Z","shell.execute_reply":"2024-10-14T13:04:30.960742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 줄인 데이터 확인\nprint(final_df['분류'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.962951Z","iopub.status.idle":"2024-10-14T13:04:30.963326Z","shell.execute_reply.started":"2024-10-14T13:04:30.963148Z","shell.execute_reply":"2024-10-14T13:04:30.963167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Dataset","metadata":{}},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        label = self.labels[item] if self.labels is not None else -1\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'text': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.965047Z","iopub.status.idle":"2024-10-14T13:04:30.965528Z","shell.execute_reply.started":"2024-10-14T13:04:30.965273Z","shell.execute_reply":"2024-10-14T13:04:30.965296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# # 데이터 준비\n# train_df['제목_키워드'] = train_df['제목'] + ' ' + train_df['키워드']\n# test_df['제목_키워드'] = test_df['제목'] + ' ' + test_df['키워드']\n\n# # 레이블 인코딩\n# label_encoder = {label: i for i, label in enumerate(train_df['분류'].unique())}\n# train_df['label'] = train_df['분류'].map(label_encoder)\n\n# # 데이터 분할 (train -> train + validation)\n# train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['분류'], random_state=42)\n\n# # 데이터셋 생성\n# train_dataset = TextDataset(train_df.제목_키워드.tolist(), train_df.label.tolist(), tokenizer)\n# val_dataset = TextDataset(val_df.제목_키워드.tolist(), val_df.label.tolist(), tokenizer)\n# test_dataset = TextDataset(test_df.제목_키워드.tolist(), None, tokenizer)  # 라벨 없음\n\n# # 데이터 로더 생성\n# train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True)\n# val_loader = DataLoader(val_dataset, batch_size=CFG.batch_size, shuffle=False)\n# test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.966556Z","iopub.status.idle":"2024-10-14T13:04:30.967061Z","shell.execute_reply.started":"2024-10-14T13:04:30.966783Z","shell.execute_reply":"2024-10-14T13:04:30.966806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n    \"learning_rate\": 0.001,   # 학습률\n    \"epoch\": 15,               # 에폭 수\n    \"batch_size\": 64,          # 배치 크기\n    \"momentum\": 0.9,           # 모멘텀\n    \"weight_decay\": 0.0005,    # 가중치 감쇠\n    \"dropout_rate\": 0.5,       # 드롭아웃 비율\n    \"gradient_clipping\": 1.0   # 그래디언트 클리핑\n}\n\nCFG = SimpleNamespace(**config)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.969285Z","iopub.status.idle":"2024-10-14T13:04:30.969618Z","shell.execute_reply.started":"2024-10-14T13:04:30.969446Z","shell.execute_reply":"2024-10-14T13:04:30.969463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.970576Z","iopub.status.idle":"2024-10-14T13:04:30.970908Z","shell.execute_reply.started":"2024-10-14T13:04:30.970729Z","shell.execute_reply":"2024-10-14T13:04:30.970759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n#tokenizer = BertTokenizer.from_pretrained('skt/kobert-base-v1')\n#tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.972234Z","iopub.status.idle":"2024-10-14T13:04:30.972626Z","shell.execute_reply.started":"2024-10-14T13:04:30.972412Z","shell.execute_reply":"2024-10-14T13:04:30.972437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 모델 초기화 (드롭아웃 비율 적용)\n# model = BertForSequenceClassification.from_pretrained(\n#     'monologg/kobert', \n#     config=config\n# ).to(device)\nmodel = BertForSequenceClassification.from_pretrained('skt/kobert-base-v1', \n                                                      config=config\n                                                        ).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.973660Z","iopub.status.idle":"2024-10-14T13:04:30.973984Z","shell.execute_reply.started":"2024-10-14T13:04:30.973813Z","shell.execute_reply":"2024-10-14T13:04:30.973836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 데이터 준비\nfinal_df['제목_키워드'] = final_df['제목'] + ' ' + final_df['키워드']\ntest_df['제목_키워드'] = test_df['제목'] + ' ' + test_df['키워드']\n\n# 레이블 인코딩\nlabel_encoder = {label: i for i, label in enumerate(final_df['분류'].unique())}\nfinal_df['label'] = final_df['분류'].map(label_encoder)\n\n# 데이터 분할 (final_df -> train + validation)\ntrain_df, val_df = train_test_split(final_df, test_size=0.2, stratify=final_df['분류'], random_state=42)\n\n# 데이터셋 생성\ntrain_dataset = TextDataset(train_df.제목_키워드.tolist(), train_df.label.tolist(), tokenizer)\nval_dataset = TextDataset(val_df.제목_키워드.tolist(), val_df.label.tolist(), tokenizer)\ntest_dataset = TextDataset(test_df.제목_키워드.tolist(), None, tokenizer)  # 라벨 없음\n\n# 데이터 로더 생성\ntrain_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=CFG.batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.975356Z","iopub.status.idle":"2024-10-14T13:04:30.975715Z","shell.execute_reply.started":"2024-10-14T13:04:30.975533Z","shell.execute_reply":"2024-10-14T13:04:30.975552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#모델 2","metadata":{}},{"cell_type":"code","source":"# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# #tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n# model = BertForSequenceClassification.from_pretrained(\n#     'monologg/kobert',\n#     num_labels=len(train_df['분류'].unique())).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.976775Z","iopub.status.idle":"2024-10-14T13:04:30.977095Z","shell.execute_reply.started":"2024-10-14T13:04:30.976931Z","shell.execute_reply":"2024-10-14T13:04:30.976948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #옵티마이저 및 학습 파라미터 설정\n# optimizer = AdamW(model.parameters(), lr=CFG.learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.978187Z","iopub.status.idle":"2024-10-14T13:04:30.978515Z","shell.execute_reply.started":"2024-10-14T13:04:30.978348Z","shell.execute_reply":"2024-10-14T13:04:30.978365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # 학습\n# model.train()\n# #for epoch in range(CFG.epoch):\n# for epoch in range(CFG.epoch):\n#     for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{CFG.epoch}'):\n#         optimizer.zero_grad()\n#         input_ids = batch['input_ids'].to(device)\n#         attention_mask = batch['attention_mask'].to(device)\n#         labels = batch['labels'].to(device)\n#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n#         loss = outputs.loss\n#         loss.backward()\n#         optimizer.step()\n\n#     # Validation\n#     model.eval()\n#     val_predictions = []\n#     val_true_labels = []\n#     with torch.no_grad():\n#         for batch in tqdm(val_loader, desc='Validating'):\n#             input_ids = batch['input_ids'].to(device)\n#             attention_mask = batch['attention_mask'].to(device)\n#             labels = batch['labels'].to(device)\n#             outputs = model(input_ids, attention_mask=attention_mask)\n#             _, preds = torch.max(outputs.logits, dim=1)\n#             val_predictions.extend(preds.cpu().tolist())\n#             val_true_labels.extend(labels.cpu().tolist())\n    \n#     # 검증 결과 출력\n#     val_f1 = f1_score(val_true_labels, val_predictions, average='macro')\n#     print(f\"Validation F1 Score: {val_f1:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.980165Z","iopub.status.idle":"2024-10-14T13:04:30.980643Z","shell.execute_reply.started":"2024-10-14T13:04:30.980385Z","shell.execute_reply":"2024-10-14T13:04:30.980409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#모델 2","metadata":{}},{"cell_type":"code","source":"# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# # 토크나이저 초기화 (BertTokenizer 사용)\n# #tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n\n# # BERT 구성 설정에서 드롭아웃 비율 변경 (예: 0.3으로 설정)\n# config = BertConfig.from_pretrained('monologg/kobert', hidden_dropout_prob=0.3, num_labels=len(train_df['분류'].unique()))\n\n# # 모델 초기화 (드롭아웃 비율 적용)\n# model = BertForSequenceClassification.from_pretrained(\n#     'monologg/kobert', \n#     config=config\n# ).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.982377Z","iopub.status.idle":"2024-10-14T13:04:30.982851Z","shell.execute_reply.started":"2024-10-14T13:04:30.982606Z","shell.execute_reply":"2024-10-14T13:04:30.982630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score\n\n# 모델 학습 함수\ndef train_and_validate(model, train_loader, val_loader, optimizer, scheduler, CFG, device):\n    model.train()\n    best_val_f1 = 0\n    patience = 3\n    early_stopping_counter = 0\n\n    for epoch in range(CFG.epoch):\n        # Training\n        model.train()\n        total_loss = 0\n        for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{CFG.epoch}'):\n            optimizer.zero_grad()\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            # Forward pass\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            loss.backward()\n            \n            # Gradient Clipping\n            #torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.gradient_clipping)\n            \n            optimizer.step()\n            total_loss += loss.item()\n        \n        avg_train_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch + 1}/{CFG.epoch}, Training Loss: {avg_train_loss:.4f}\")\n        \n        # Validation\n        model.eval()\n        val_predictions = []\n        val_true_labels = []\n        val_loss = 0\n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc='Validating'):\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                \n                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n                val_loss += outputs.loss.item()\n                _, preds = torch.max(outputs.logits, dim=1)\n                val_predictions.extend(preds.cpu().tolist())\n                val_true_labels.extend(labels.cpu().tolist())\n        \n        avg_val_loss = val_loss / len(val_loader)\n        val_f1 = f1_score(val_true_labels, val_predictions, average='macro')\n        \n        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation F1 Score: {val_f1:.2f}\")\n        \n        # Early Stopping\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            early_stopping_counter = 0\n        else:\n            early_stopping_counter += 1\n            if early_stopping_counter >= patience:\n                print(\"Early stopping triggered!\")\n                break\n        \n        # Learning Rate Scheduler step\n        scheduler.step()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.984084Z","iopub.status.idle":"2024-10-14T13:04:30.984559Z","shell.execute_reply.started":"2024-10-14T13:04:30.984320Z","shell.execute_reply":"2024-10-14T13:04:30.984345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=CFG.learning_rate, weight_decay=CFG.weight_decay)\nscheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # 매 5 에폭마다 학습률을 10% 감소\n\n# Train and validate (train_loader와 val_loader가 이미 정의되어 있다고 가정)\ntrain_and_validate(model, train_loader, val_loader, optimizer, scheduler, CFG, device)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.985949Z","iopub.status.idle":"2024-10-14T13:04:30.986306Z","shell.execute_reply.started":"2024-10-14T13:04:30.986106Z","shell.execute_reply":"2024-10-14T13:04:30.986148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"model.eval()\ntest_predictions = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc='Testing'):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        \n        # 모델을 통한 예측\n        outputs = model(input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs.logits, dim=1)\n        \n        # 예측 결과 저장\n        test_predictions.extend(preds.cpu().tolist())\n\n# test.csv 파일에서 'id' 값을 그대로 사용\ntest_df = pd.read_csv('/kaggle/input/dacon-dataset/test.csv')  # test.csv에 'id' 칼럼이 존재한다고 가정\n\n# 라벨 디코딩\nlabel_decoder = {i: label for label, i in label_encoder.items()}\ndecoded_predictions = [label_decoder[pred] for pred in test_predictions]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.987850Z","iopub.status.idle":"2024-10-14T13:04:30.988201Z","shell.execute_reply.started":"2024-10-14T13:04:30.988007Z","shell.execute_reply":"2024-10-14T13:04:30.988024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 예측 결과를 데이터프레임으로 저장\ndf_results = pd.DataFrame({\n    'ID': test_df['ID'],             # test.csv의 'id' 값을 그대로 사용\n    '분류': decoded_predictions  # 디코딩된 예측 라벨\n})\n\n# CSV 파일로 저장\ndf_results.to_csv('/kaggle/working/submission.csv', index=False)\n\nprint(\"예측 결과가 submission.csv 파일로 저장되었습니다.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.989409Z","iopub.status.idle":"2024-10-14T13:04:30.989761Z","shell.execute_reply.started":"2024-10-14T13:04:30.989593Z","shell.execute_reply":"2024-10-14T13:04:30.989610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/working/submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.990708Z","iopub.status.idle":"2024-10-14T13:04:30.991078Z","shell.execute_reply.started":"2024-10-14T13:04:30.990896Z","shell.execute_reply":"2024-10-14T13:04:30.990914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(50)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:04:30.992862Z","iopub.status.idle":"2024-10-14T13:04:30.993341Z","shell.execute_reply.started":"2024-10-14T13:04:30.993080Z","shell.execute_reply":"2024-10-14T13:04:30.993103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}